<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Neural Experts: Mixture of Experts for Implicit Neural Representations</title>
    <meta name="author"
          content="Yizhak Ben-Shabat (Itzik), Chamin Hewa Koneputugodage, Sameera Ramasinghe, and Stephen Gould">
    <meta name="description"
          content="Neural Experts NeurIPS paper project page">
    <meta name="keywords" content="Neural Experts,  INR, Surface Reconstruction, Point Clouds, Mixture of Experts, MoE">

    <link rel="stylesheet" href="./css/index.css">
</head>
<body>

<section class="hero">
    <div class="default-container">
        <div class="title-block">
            <h1 class="title">Neural Experts: Mixture of Experts for Implicit Neural Representations</h1>
            <h2 class="venue">NeurIPS 2024</h2>
        </div>
        <div class="author-section">
            <span class="author">
                <a href="https://www.itzikbs.com/">
                    <img class="image" src="./images/itzik.jpg" alt="Yizhak Ben-Shabat (Itzik)">
                </a> <br/>
                <a href="https://www.itzikbs.com/">Yizhak Ben-Shabat <br> (Itzik)*</a><sup>1</sup>
            </span>
            <span class="author">
                <a href="https://www.linkedin.com/in/chamin-hewa-koneputugodage-b3ba17148/">
                    <img class="image" src="./images/chamin.jpg" alt="Chamin Hewa Koneputugodage">
                </a> <br/>
                <a href="https://www.linkedin.com/in/chamin-hewa-koneputugodage-b3ba17148/">Chamin Hewa
                    <br> Koneputugodage*</a><sup>1</sup>
            </span>
            <span class="author">
                <a href="https://www.linkedin.com/in/sameeraramasinghe/)">
                    <img class="image" src="./images/sameera.jpg" alt="Sameera Ramasinghe">
                </a> <br/>
                <a href="https://www.linkedin.com/in/sameeraramasinghe/">Sameera Ramasinghe </a><sup>2</sup>
            </span>
            <span class="author">
                <a href="https://cecs.anu.edu.au/people/stephen-gould/">
                    <img class="image" src="./images/steve.jpg" alt="Stephen Gould">
                </a> <br/>
                <a href="https://cecs.anu.edu.au/people/stephen-gould/">Stephen Gould </a><sup>1</sup><br><a>&nbsp</a>
            </span>
        </div>

        <div class="affiliation">
            <sup>1</sup><a>The Australian National University</a>
            &emsp;
            <sup>2</sup><a>Australian Institute of Machine Learning</a><br>
            <a>*Equal Contribution</a>
        </div>

        <div class="links">
            <span class="link-block">
                <a href="https://arxiv.org/pdf/2106.10811.pdf" class="button-80">
                    <img class="link-icon" src="./images/pdf.svg" alt="Paper">
                    <span> Paper</span>
                </a>
            </span>
            <span class="link-block">
                <a href="https://arxiv.org/abs/2106.10811" class="button-80">
                    <img class="link-icon" src="./images/arxiv.png" alt="arxiv">
                    <span> Arxiv</span>
                </a>
            </span>
            <!-- <span class="link-block">
                <a href="coming_soon.html" class="button-80">
                    <img class="link-icon" src="./images/jupyter.svg" alt="Jupter Notebook">
                    <span> Jupter Notebook</span>
                </a>
            </span> -->
            <span class="link-block">
                <a href="https://github.com/sitzikbs/Neural-Experts" class="button-80">
                    <img class="link-icon" src="./images/github.svg" alt="Code">
                    <span> Code</span>
                </a>
            </span>
            <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=bQWpRyM9wYM&list=PLD-7XrNHCcFLDQ6KH7w2xFLe0JWvalYfr&index=3&ab_channel=anucvml" class="button-80">
                    <img class="link-icon" src="./images/youtube.svg" alt="Video">
                    <span> Video</span>
                </a>
            </span> -->
            <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=w2xC2JluMlk&ab_channel=TalkingPapersPodcast" class="button-80">
                    <img class="link-icon" src="./images/youtube.svg" alt="Video">
                    <span> Podcast</span>
                </a>
            </span> -->
        </div>

    </div>
</section>

<section class="teaser-results">
    <div class="default-container">
        <div class="srb-comp-row">
            <div class="srb-vid-column" style="overflow: hidden;">
              <div class="divergence-expl">
                  <img class="divergence-expl-img" src="./images/inr.png" alt="teaser">
              </div>
            </div>
            <div class="srb-vid-column" style="overflow: hidden;">
              <div class="divergence-expl">
                  <img class="divergence-expl-img" src="./images/inr_moe.png" alt="teaser">
              </div>
            </div>
            <div class="srb-vid-column" style="overflow: hidden;">
              <div class="divergence-expl">
                  <img class="divergence-expl-img" src="./images/neural_experts.png" alt="teaser">
              </div>
            </div>
        </div>
        <p>
          Illustration comparing between INR architectures for traditional INR (left), Vanilla MoE (middle)
          INR and the proposed Neural Experts (right). Two key elements of our approach include the conditioning
          and pretraining of the manager that improve signal reconstruction with fewer parameters.
        </p>

    </div>
</section>


<section class="abstract-section">
    <div class="default-container">
        <div class="abstract">
            <h2 class="abstract-title">Abstract</h2>
            <p class="abstract-text">
              Implicit neural representations (INRs) have proven effective in various tasks including image, shape, audio, and video reconstruction.
              These INRs typically learn the implicit field from sampled input points. This is often done using a single network for the entire domain,
              imposing many global constraints on a single function. In this paper, we propose a mixture of experts (MoE) implicit neural representation
              approach that enables learning local piece-wise continuous functions that simultaneously learns to subdivide the domain and fit locally.
              We show that incorporating a mixture of experts architecture into existing INR formulations provides a boost in speed, accuracy, and memory requirements.
              Additionally, we introduce novel conditioning and pretraining methods for the gating network that improves convergence to the desired solution.
              We evaluate the effectiveness of our approach on multiple reconstruction tasks, including surface reconstruction, image reconstruction,
              and audio signal reconstruction and show improved performance compared to non-MoE methods.
            </p>
        </div>
    </div>
</section>

<section class="main-video-section">
    <div class="default-container">
        <div class="main-video">
            <h2 class="main-video-title">Video</h2>
            <!-- <div class="main-video-div">
                <iframe src="https://www.youtube.com/embed/bQWpRyM9wYM?rel=0&amp;showinfo=0"
                        frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
            </div> -->
            <p style="text-align: justify"><b>Coming Soon!</b></p>
        </div>
    </div>

<section class="explanation-section">
    <div class="default-container">
        <div class="explanation">
            <h2 class="explanation-title">Mixture of Experts for INRs</h2>



            <br>
            <h3>The manager network</h3>
            <p>
              The manager network provides more control over the learned representation.
               A manager's assignment change can accomodiate sharp discontinuity in the data.
               It can also be pretrained using some prior (e.g. segmentation). Our experiments show that a random manager initialization performs best as the underlaying segmentation appears naturally from the training process.
              <img class="image" src="./images/manager_pretraining.jpg" alt="manager_pretraining">
              Manager pretraining. Visualizing the experts selected by the manager after the pretraining
              stage (top row) and after the full network training (bottom row) for different pretraining ablations.
            </p>
            <h3>Additional Results</h3>
            
            <img class="image" src="./images/3d_recon_results.jpg" alt="3d reconstruction results">
            <strong>3D surface reconstruction. </strong> Results on the Thai Statue shape. Our method noticeably
            captures more detail in the toes, nostrils, and eye. The error colormap shows that our method produces
            a mesh with far less large errors (lighter indicates higher distance to the ground truth surface), and
            the expert segmentation shows our method provides a subdivision of the space.

        </div>
    </div>
</section>

<!-- <section class="thanks-section">
    <div class="default-container">
        <div class="thanks">
            <h2 class="thanks-title">Acknowledgements</h2>
            <p class="thanks-text">
                This project has received funding from the European Union's Horizon 2020 research and innovation
                programme under the Marie Sklodowska-Curie grant agreement No 893465
            </p>
        </div>
    </div>
</section> -->

<section class="reference-section">
    <div class="default-container">
        <div class="ref">
            <h2 class="ref-title">BibTeX</h2>
            <div class="bibtex">
<pre><code>@inproceedings{ben2024neuralexperts,
    title={Neural Experts: Mixture of Experts for Implicit Neural Representations},
    author={Ben-Shabat, Yizhak and Hewa Koneputugodage, Chamin and Ramasinghe, Sameera and Gould, Stephen},
    booktitle={Advances in Neural Information Processing Systems (NeurIPS},
    year={2024}
  }</code></pre>
            </div>
        </div>
    </div>
</section>

</body>
</html>
